{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27617e-f38b-4907-8405-87f94aae4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from rouge_score import rouge_scorer  # Google's Rouge implementation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "import evaluate  # Huggingface wrapper around Google's Rouge implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088bee1-7f12-4987-886c-5769fbc6c41f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Auxiliary functions\n",
    "We additionally include Precision, Recall, F1 via strict or relaxed operators, and alternative ways to induce boundaries within triples in ROUGE. In the experiments, we only tested and evaluated ROUGE via space separators (default configuration in its implementation by Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249e1d8-c3a7-44fe-9fed-69fd2ba7caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for allowing comparing triples via equality that also\n",
    "# indicates how we should verbalize them\n",
    "class Triple:\n",
    "    full_equality = True\n",
    "    verbalize_with_random_separators = True\n",
    "\n",
    "    def __init__(self, s, p, o):\n",
    "        self.elements = (s, p, o)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Triple):\n",
    "            s1, p1, o1 = self.elements\n",
    "            s2, p2, o2 = other.elements\n",
    "\n",
    "            if Triple.full_equality:\n",
    "                return s1 == s2 and p1 == p2 and o1 == o2\n",
    "            else:\n",
    "                # Allow triple elements to be partially\n",
    "                # contained inside their equivalent s\n",
    "                return (\n",
    "                        (s1 == s2 and p1 == p2 and o1 == o2) or\n",
    "                        (\n",
    "                                (s1 in s2 or s2 in s1) and\n",
    "                                (p1 in p2 or p2 in p1) and\n",
    "                                (o1 in o2 or o2 in o1)\n",
    "                        )\n",
    "                )\n",
    "        return False\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.elements[index]\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260cb701-9fa2-4ab7-9182-4abaa0453dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a random valid unicode char not within the provided set\n",
    "def get_random_char(forbidden_chars):\n",
    "    unicode_start_range = 0x0020  # Start of printable characters range\n",
    "    unicode_end_range = 0x10FFFF  # End of unicode range\n",
    "\n",
    "    random_unicode_char = chr(random.randint(unicode_start_range, unicode_end_range))\n",
    "\n",
    "    if random_unicode_char not in forbidden_chars:\n",
    "        return random_unicode_char\n",
    "    else:\n",
    "        return get_random_char(forbidden_chars)\n",
    "    \n",
    "# Adds to forbidden_chars the characters containined in each triple,\n",
    "# so that when we use random separators, they don't contain them\n",
    "def get_chars_from_triples(forbidden_chars, triples_list):\n",
    "    for triples in triples_list:\n",
    "        for (s, p, o) in triples:\n",
    "            for char in s:\n",
    "                forbidden_chars.add(char)\n",
    "            for char in p:\n",
    "                forbidden_chars.add(char)\n",
    "            for char in o:\n",
    "                forbidden_chars.add(char)\n",
    "\n",
    "    return forbidden_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ec138-d467-446d-b962-f151447a1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns the two sets of triples into text following the Triple's configuration\n",
    "def verbalize_triples(generated_triples, ground_truth):\n",
    "    # First, we build a set of characters already present in the triples, which we should not\n",
    "    # use as random separators. We will add random chars as separators between triples, ensuring\n",
    "    # that they  did not appear either in the triples or in previous separators\n",
    "    forbidden_chars = set()\n",
    "\n",
    "    if Triple.verbalize_with_random_separators:\n",
    "        forbidden_chars = get_chars_from_triples(forbidden_chars, ground_truth)\n",
    "        forbidden_chars = get_chars_from_triples(forbidden_chars, generated_triples)\n",
    "\n",
    "    lines_gt = []\n",
    "    lines_generated = []\n",
    "\n",
    "    # Every sample is a verbalized string of all tuples, separated\n",
    "    # by a special symbol, which is different for ground and generated\n",
    "    # triples in order to ensure that there are no matches across different\n",
    "    # triples\n",
    "    for triples in ground_truth:\n",
    "        line_gt = \" \"\n",
    "        for i, (s, p, o) in enumerate(triples):\n",
    "            line_gt += f\"{s} {p} {o}\"\n",
    "            if i < len(triples) - 1:\n",
    "                if Triple.verbalize_with_random_separators:\n",
    "                    random_char = get_random_char(forbidden_chars)\n",
    "                    line_gt += f\" {random_char} \"\n",
    "                    forbidden_chars.add(random_char)\n",
    "                else:\n",
    "                    line_gt += \" \"\n",
    "\n",
    "        lines_gt.append(line_gt)\n",
    "\n",
    "    for triples in generated_triples:\n",
    "        line_generated = \" \"\n",
    "        try:\n",
    "            for i, (s, p, o) in enumerate(triples):\n",
    "                line_generated += f\"{s} {p} {o}\"\n",
    "                if Triple.verbalize_with_random_separators:\n",
    "                    random_char = get_random_char(forbidden_chars)\n",
    "                    line_generated += f\" {random_char} \"\n",
    "                    forbidden_chars.add(random_char)\n",
    "                else:\n",
    "                    line_generated += \" \"\n",
    "\n",
    "        except Exception as e:  # One of the triples was wrong (missing object usually)\n",
    "            line_generated = \" \"\n",
    "\n",
    "        lines_generated.append(line_generated)\n",
    "\n",
    "    return lines_gt, lines_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230cb10-415a-407a-ac80-0957268a1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge and P/R/F1 functions\n",
    "\n",
    "def evaluate_text_huggingface_impl(ground_truth, generated_triples):\n",
    "    lines_gt, lines_generated = verbalize_triples(generated_triples, ground_truth)\n",
    "    results = evaluate.load('rouge').compute(predictions=lines_generated, references=lines_gt)\n",
    "\n",
    "    return results[\"rouge1\"], results[\"rouge2\"], results[\"rougeL\"]\n",
    "\n",
    "\n",
    "def evaluate_text_google_impl(ground_truth, generated_triples, custom_tokenizer):\n",
    "    lines_gt, lines_generated = verbalize_triples(generated_triples, ground_truth)\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'],\n",
    "                                      use_stemmer=False,\n",
    "                                      split_summaries=False,\n",
    "                                      # For example, from NLTK. This ensures that\n",
    "                                      # no regexes or other filtering rules are used\n",
    "                                      #\n",
    "                                      # Their only requirement is that the class has\n",
    "                                      # a tokenize() method that does anything and\n",
    "                                      # returns a list of strings\n",
    "                                      #\n",
    "                                      # use_stemmer won't have any effect if we are\n",
    "                                      # using a custom tokenizer\n",
    "                                      tokenizer=custom_tokenizer)\n",
    "    scores_rouge_1 = []\n",
    "    scores_rouge_2 = []\n",
    "    scores_rouge_L = []\n",
    "    for line_g, line_gt in zip(lines_generated, lines_gt):\n",
    "        scores = scorer.score(line_gt, line_g)\n",
    "        p_r1, r_r1, f1_r1 = scores['rouge1']\n",
    "        p_r2, r_r2, f1_r2 = scores['rouge2']\n",
    "        p_rL, r_rL, f1_rL = scores['rougeL']\n",
    "        scores_rouge_1.append(f1_r1)\n",
    "        scores_rouge_2.append(f1_r2)\n",
    "        scores_rouge_L.append(f1_rL)\n",
    "\n",
    "    avg_rouge_1 = sum(scores_rouge_1) / len(scores_rouge_1)\n",
    "    avg_rouge_2 = sum(scores_rouge_2) / len(scores_rouge_2)\n",
    "    avg_rouge_L = sum(scores_rouge_L) / len(scores_rouge_L)\n",
    "\n",
    "    return avg_rouge_1, avg_rouge_2, avg_rouge_L\n",
    "\n",
    "\n",
    "def calculate_tp_fp_fn(ground_truth_triples, gen_triples):\n",
    "    if len(ground_truth_triples) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    # We allow partial equalities and it's a list of singletons\n",
    "    # (we are comparing subjects, predicates or objects separately)\n",
    "    #\n",
    "    # If not, the implementation below works for any other case\n",
    "    if not Triple.full_equality and type(ground_truth_triples[0]) is str:\n",
    "        for singleton_gen in gen_triples:\n",
    "            found = False\n",
    "            for singleton_gt in ground_truth_triples:\n",
    "                if singleton_gen in singleton_gt or singleton_gt in singleton_gen:\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "            if found:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "\n",
    "    # We don't do it via set operators since we want to use the custom\n",
    "    # equality functions, not hashes\n",
    "    for gen_triple in gen_triples:\n",
    "        if gen_triple in ground_truth_triples:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "\n",
    "    if len(ground_truth_triples) > len(gen_triples):\n",
    "        false_negative = len(ground_truth_triples) - len(gen_triples) # Triples that we missed, otherwise 0\n",
    "\n",
    "    return true_positive, false_positive, false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18998607-2602-4f19-a656-0bf02c6cdd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns triple comparison results\n",
    "def evaluate_triples(ground_truth, generated_triples):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    for ground_truth_triples, gen_triples in zip(ground_truth, generated_triples):\n",
    "        # Only zephyr causes this: It returns a list of tuples of arrays\n",
    "        for t in gen_triples:\n",
    "            try:\n",
    "                hash(t)\n",
    "            except TypeError:\n",
    "                gen_triples = []\n",
    "                break\n",
    "\n",
    "        true_positive, false_positive, false_negative = calculate_tp_fp_fn(ground_truth_triples, gen_triples)\n",
    "\n",
    "        precision = true_positive / max(1, true_positive + false_positive)\n",
    "        recall = true_positive / max(1, true_positive + false_negative)\n",
    "        f1 = 2 * (precision * recall) / max(1e-10, precision + recall)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    p = sum(precision_scores) / len(precision_scores)\n",
    "    r = sum(recall_scores) / len(recall_scores)\n",
    "    f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef48dc-43f1-42cd-8741-abe4a3439c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects(triples):\n",
    "    return [t[0] for sublist in triples for t in sublist]\n",
    "\n",
    "\n",
    "def get_predicates(triples):\n",
    "    return [t[1] for sublist in triples for t in sublist]\n",
    "\n",
    "\n",
    "def get_objects(triples):\n",
    "    return [t[2] for sublist in triples for t in sublist]\n",
    "\n",
    "\n",
    "def is_valid_triple(t):\n",
    "    return isinstance(t, tuple) and len(t) == 3\n",
    "\n",
    "\n",
    "def clean_and_turn_to_triple(t):\n",
    "    s, p, o = t\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    if not isinstance(p, str):\n",
    "        p = str(p)\n",
    "    if not isinstance(o, str):\n",
    "        o = str(o)\n",
    "\n",
    "    return Triple(s.lower(), p.lower(), o.lower())\n",
    "\n",
    "\n",
    "# Given a results file from LLM_testing, writes to the CSV file the comparison results for\n",
    "# every possible configuration (full or relaxed triple equality, random separators in rouge or not...)\n",
    "def eval_results_file(path, LLM_name, n_samples, out_csv_file):\n",
    "    with open(path, 'r') as file:\n",
    "        samples = []\n",
    "        ground_truth = []\n",
    "        generated_triples = []\n",
    "        invalid_generations = 0\n",
    "\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Sample: \"):\n",
    "                samples.append(line[len(\"Sample: \"):])\n",
    "\n",
    "            if line.startswith(\"Ground truth: \"):\n",
    "                gt_triples = eval(line[len(\"Ground truth: \"):])\n",
    "                ground_truth.append([clean_and_turn_to_triple(t) for t in gt_triples])\n",
    "\n",
    "            if line.startswith(\"Generated triples: \"):\n",
    "                try:\n",
    "                    gen_triples_raw = []\n",
    "                    for gen_triple_raw in eval(line[len(\"Generated triples: \"):]):\n",
    "                        if is_valid_triple(gen_triple_raw):\n",
    "                            gen_triples_raw.append(gen_triple_raw)\n",
    "                        #else:\n",
    "                            #print(f\"{LLM_name}: invalid generated triple {gen_triple_raw}\")\n",
    "\n",
    "                    gt = [clean_and_turn_to_triple(t) for t in gen_triples_raw]\n",
    "                    generated_triples.append(gt)\n",
    "                    if len(gt) == 0:\n",
    "                        invalid_generations += 1\n",
    "\n",
    "                except Exception as e:  # Only happens with webNLG (it sometimes generates triples without objects)\n",
    "                    invalid_generations += 1\n",
    "                    generated_triples.append([])\n",
    "\n",
    "    subjects_generated = get_subjects(generated_triples)\n",
    "    subjects_ground_truth = get_subjects(ground_truth)\n",
    "\n",
    "    predicates_generated = get_predicates(generated_triples)\n",
    "    predicates_ground_truth = get_predicates(ground_truth)\n",
    "\n",
    "    objects_generated = get_objects(generated_triples)\n",
    "    objects_ground_truth = get_objects(ground_truth)\n",
    "\n",
    "    Triple.full_equality = True\n",
    "    p_fe, r_fe, f1_fe = evaluate_triples(ground_truth, generated_triples)\n",
    "    p_s_fe, r_s_fe, f1_s_fe = evaluate_triples(subjects_ground_truth, subjects_generated)\n",
    "    p_p_fe, r_p_fe, f1_p_fe = evaluate_triples(predicates_ground_truth, predicates_generated)\n",
    "    p_o_fe, r_o_fe, f1_o_fe = evaluate_triples(objects_ground_truth, objects_generated)\n",
    "\n",
    "    Triple.full_equality = False\n",
    "    p_relaxed, r_relaxed, f1_relaxed = evaluate_triples(ground_truth, generated_triples)\n",
    "    p_s_relaxed, r_s_relaxed, f1_s_relaxed = evaluate_triples(subjects_ground_truth, subjects_generated)\n",
    "    p_p_relaxed, r_p_relaxed, f1_p_relaxed = evaluate_triples(predicates_ground_truth, predicates_generated)\n",
    "    p_o_relaxed, r_o_relaxed, f1_o_relaxed = evaluate_triples(objects_ground_truth, objects_generated)\n",
    "\n",
    "    Triple.verbalize_with_random_separators = False\n",
    "    avg_rouge_1_google_space_seps, avg_rouge_2_google_space_seps, avg_rouge_L_google_space_seps = (\n",
    "        evaluate_text_google_impl(ground_truth,\n",
    "                                  generated_triples,\n",
    "                                  None))\n",
    "\n",
    "    Triple.verbalize_with_random_separators = True\n",
    "    avg_rouge_1_google_random_seps, avg_rouge_2_google_random_seps, avg_rouge_L_google_random_seps = (\n",
    "        evaluate_text_google_impl(ground_truth,\n",
    "                                  generated_triples,\n",
    "                                  None))\n",
    "\n",
    "    Triple.verbalize_with_random_separators = False\n",
    "    avg_rouge_1_google_space_seps_custom_tokenizer, avg_rouge_2_google_space_seps_custom_tokenizer, avg_rouge_L_google_space_seps_custom_tokenizer = (\n",
    "        evaluate_text_google_impl(ground_truth,\n",
    "                                  generated_triples,\n",
    "                                  WhitespaceTokenizer()))\n",
    "\n",
    "    Triple.verbalize_with_random_separators = True\n",
    "    avg_rouge_1_google_random_seps_custom_tokenizer, avg_rouge_2_google_random_seps_custom_tokenizer, avg_rouge_L_google_random_seps_custom_tokenizer = (\n",
    "        evaluate_text_google_impl(ground_truth,\n",
    "                                  generated_triples,\n",
    "                                  WhitespaceTokenizer()))\n",
    "\n",
    "    out_csv_file.write(f\"{LLM_name},\")\n",
    "    out_csv_file.write(f\"{n_samples},\")\n",
    "\n",
    "    out_csv_file.write(f\"{invalid_generations},\")\n",
    "\n",
    "    out_csv_file.write(f\"{round(p_fe, 3)},\")\n",
    "    out_csv_file.write(f\"{round(p_relaxed, 3)},\")\n",
    "\n",
    "    out_csv_file.write(f\"{round(f1_fe, 3)},\")\n",
    "    out_csv_file.write(f\"{round(f1_relaxed, 3)},\")\n",
    "\n",
    "    out_csv_file.write(f\"{round(f1_s_fe, 3)},\")\n",
    "    out_csv_file.write(f\"{round(f1_s_relaxed, 3)},\")\n",
    "\n",
    "    out_csv_file.write(f\"{round(f1_p_fe, 3)},\")\n",
    "    out_csv_file.write(f\"{round(f1_p_relaxed, 3)},\")\n",
    "\n",
    "    out_csv_file.write(f\"{round(f1_o_fe, 3)},\")\n",
    "    out_csv_file.write(f\"{round(f1_o_relaxed, 3)},\")\n",
    "\n",
    "    out_csv_file.write(f\"{round(avg_rouge_1_google_space_seps, 3)},\")\n",
    "    out_csv_file.write(f\"{round(avg_rouge_2_google_space_seps, 3)},\")\n",
    "    out_csv_file.write(f\"{round(avg_rouge_L_google_space_seps, 3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c4982-31ea-4aca-b818-318721bab78f",
   "metadata": {},
   "source": [
    "# Evaluate the results\n",
    "The results will be saved to `results_llm_testing/results.csv`, containing the F1 and Rouge scores\n",
    "using different configurations:\n",
    "- Strict F1 (ommited): Calculate F1 scores by matching verbalized triples (`\"{s} {p} {o}\"`) via a strict string equality\n",
    "- Relaxed F1 (ommited): Calculate the same score, but allowing subjects, predicates and objects to be partially within the ground truth or viceversa\n",
    "- Space separators: Separate the verbalized triples of every sample via spaces (`\"{s1} {p1} {o1} {s2} {p2} {o2} ...\"`)\n",
    "- Random separators (ommited): Instead of spaces, add random unicode separators when verbalizing triples. This makes it impossible for rouge-2 and rouge-L to go beyond\n",
    "  the boundaries of single triple (otherwise, for example, given (s1,p1,o1) and (s2,p2,o2), it could match (p1,o1,s2))\n",
    "- Custom tokenizer (ommited): Instead of using the Rouge's implementation tokenizer (https://github.com/google-research/google-research/blob/master/rouge/tokenize.py), which converts to lowercase and removes non-alphanumeric characters, use NLTK's `WhiteSpaceTokenizer()` (Note: we also convert to lowercase both the generated and the ground truth triples). The tokenizer can be changed in the `eval_results_file` function\n",
    "\n",
    "The Rouge implementation being used is Google's (which is also Huggingface's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c67b2-2212-494d-8f02-d11381c1de8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import Datasets\n",
    "\n",
    "webnlg_dataset = Datasets.WebNLGDataset()\n",
    "test_samples = dict(webnlg_dataset.test_samples)\n",
    "\n",
    "def gemini_results_to_txt(gemini_results, n_samples):\n",
    "    with open(f\"results_llm_testing/results_gemini_webnlg_{n_samples}_samples.txt\", 'w') as file:\n",
    "        for text, triples in gemini_results.items():\n",
    "            sample = f\"Sample: {text}\\nGround truth: {test_samples[text]}\\nGenerated triples: {list(map(tuple, triples))}\\n\\n\"\n",
    "            file.write(sample)\n",
    "\n",
    "gemini_results_to_txt(json.load(open(\"results_llm_testing/gemini_results_5.json\", 'r')), 5)\n",
    "gemini_results_to_txt(json.load(open(\"results_llm_testing/gemini_results_8.json\", 'r')), 8)\n",
    "gemini_results_to_txt(json.load(open(\"results_llm_testing/gemini_results_16.json\", 'r')), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc84638-2f43-4d0d-934e-8f13f2dcfd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_llm_testing/results_llms.csv\", 'w') as out_csv_file:\n",
    "    # Write the CSV headers\n",
    "    out_csv_file.write(\"LLM,\")\n",
    "    out_csv_file.write(\"Examples provided,\")\n",
    "\n",
    "    out_csv_file.write(\"Invalid outputs,\")\n",
    "\n",
    "    out_csv_file.write(\"Precision (strict),\")\n",
    "    out_csv_file.write(\"Precision (relaxed),\")\n",
    "    \n",
    "    out_csv_file.write(\"F1 (strict),\")\n",
    "    out_csv_file.write(\"F1 (relaxed),\")\n",
    "\n",
    "    out_csv_file.write(\"F1 (subjects strict),\")\n",
    "    out_csv_file.write(\"F1 (subjects relaxed),\")\n",
    "\n",
    "    out_csv_file.write(\"F1 (predicates strict),\")\n",
    "    out_csv_file.write(\"F1 (predicates relaxed),\")\n",
    "\n",
    "    out_csv_file.write(\"F1 (objects strict),\")\n",
    "    out_csv_file.write(\"F1 (objects relaxed),\")\n",
    "\n",
    "    out_csv_file.write(\"Avg. Rouge-1 (space separators),\")\n",
    "    out_csv_file.write(\"Avg. Rouge-2 (space separators),\")\n",
    "    out_csv_file.write(\"Avg. Rouge-L (space separators)\\n\")\n",
    "\n",
    "    eval_results_file('results_llm_testing/results_gemma_2b_webnlg_5_samples.txt', \"Gemma-2 (2B)\", 5, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_gemma_2b_webnlg_8_samples.txt', \"Gemma-2 (2B)\", 8, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_gemma_2b_webnlg_16_samples.txt', \"Gemma-2 (2B)\", 16, out_csv_file)\n",
    "\n",
    "    eval_results_file('results_llm_testing/results_llama_3b_webnlg_5_samples.txt', \"Llama-3.2 (3B)\", 5, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_llama_3b_webnlg_8_samples.txt', \"Llama-3.2 (3B)\", 8, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_llama_3b_webnlg_16_samples.txt', \"Llama-3.2 (3B)\", 16, out_csv_file)\n",
    "\n",
    "    eval_results_file('results_llm_testing/results_phi_3b_webnlg_5_samples.txt', \"Phi-3.5 (3.8B)\", 5, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_phi_3b_webnlg_8_samples.txt', \"Phi-3.5 (3.8B)\", 8, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_phi_3b_webnlg_16_samples.txt', \"Phi-3.5 (3.8B)\", 16, out_csv_file)\n",
    "\n",
    "    eval_results_file('results_llm_testing/results_llama_8b_webnlg_5_samples.txt', \"Llama-3.1 (8B)\", 5, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_llama_8b_webnlg_8_samples.txt', \"Llama-3.1 (8B)\", 8, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_llama_8b_webnlg_16_samples.txt', \"Llama-3.1 (8B)\", 16, out_csv_file)\n",
    "\n",
    "    eval_results_file('results_llm_testing/results_gemma_9b_webnlg_5_samples.txt', \"Gemma-2 (9B)\", 5, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_gemma_9b_webnlg_8_samples.txt', \"Gemma-2 (9B)\", 8, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_gemma_9b_webnlg_16_samples.txt', \"Gemma-2 (9B)\", 16, out_csv_file)\n",
    "    \n",
    "    eval_results_file('results_llm_testing/results_gemini_webnlg_5_samples.txt', \"Gemini\", 5, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_gemini_webnlg_8_samples.txt', \"Gemini\", 8, out_csv_file)\n",
    "    eval_results_file('results_llm_testing/results_gemini_webnlg_16_samples.txt', \"Gemini\", 16, out_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c6355-ce59-41e1-9816-d7f1deec1ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda_py310]",
   "language": "python",
   "name": "conda-env-conda_py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
