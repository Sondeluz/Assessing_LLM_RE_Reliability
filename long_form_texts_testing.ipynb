{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f66a58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import LLMUtils\n",
    "import Datasets\n",
    "\n",
    "import json\n",
    "import textwrap\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import more_itertools as mit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc6aad-6125-4af6-bf12-0da28c7007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = \"\" # Needed for restricted LLMs with EULAs (Llama...)\n",
    "EVALUATE_ON_PAPERS = True # False: DocRED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f71fb-b00f-4f5c-9aaf-8a994c5fd17b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f3f8d-8a26-459b-8eac-fde95c01d1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM = LLMUtils.LLM(LLMUtils.LLM.GEMMA_9B, hf_token=hf_token) # In the publication: Gemma-2-9B, Llama-3.1-8B, Phi-3-3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d9ac8-a15c-47b3-aa26-12e09d0cc52a",
   "metadata": {},
   "source": [
    "# Load the papers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37ac5b-acd5-4b5c-b182-489f38c8df69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('papers.json') as f:\n",
    "    papers = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb833093-86c7-4158-98fc-0da346d7c617",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6e9b6-7a39-44c7-8a38-66df5c29fd41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if EVALUATE_ON_PAPERS:\n",
    "    webnlg_dataset = Datasets.WebNLGDataset() # Used for in-context samples\n",
    "    main_dataset = Datasets.WebNLGDataset() # Used for generating triples, either WebNLG (papers) or DocRED\n",
    "else:\n",
    "    webnlg_dataset = Datasets.WebNLGDataset()\n",
    "    main_dataset = Datasets.DocREDDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e73224-0842-4ae3-b5f3-b7d8a39d64c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Possible prompt strategies to do\n",
    "# PREVIOUS_SENTENCES: Split the text into chunks of n sentences,\n",
    "#                     with an overlap of m sentences which will\n",
    "#                     act as the context. n and m can be any\n",
    "#                     value (e.g. n=1, m=0 traverses it sentence\n",
    "#                     by sentence with no context). The iterator\n",
    "#                     will adjust the overlap in the first sentence(s)\n",
    "#                     where there may not be enough preceding ones\n",
    "#\n",
    "# SECTION_CONTENTS: Generate triples for the whole section at once\n",
    "PromptStrategy = Enum('PromptStrategy', ['PREVIOUS_SENTENCES', 'SECTION_CONTENTS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083ca45-c462-4679-a536-d3796e4c4735",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Auxiliary functions for post-processing and testing\n",
    "The functions below will perform triple merging when applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80202bc-156a-4cdc-bb8b-50406733ee34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Triple cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1bd87-9119-409e-b4d3-1025ae5844ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_valid_triple(t):\n",
    "    return isinstance(t, tuple) and len(t) == 3\n",
    "\n",
    "\n",
    "def clean_triple(t):\n",
    "    s, p, o = t\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    if not isinstance(p, str):\n",
    "        p = str(p)\n",
    "    if not isinstance(o, str):\n",
    "        o = str(o)\n",
    "\n",
    "    return (s, p, o)\n",
    "\n",
    "\n",
    "def clean_and_get_triples_stats(paper_triples):\n",
    "    bad_triples = []\n",
    "    total_triples = 0\n",
    "    unique_s = defaultdict(int)\n",
    "    unique_p = defaultdict(int)\n",
    "    unique_o = defaultdict(int)\n",
    "\n",
    "    for i, (sentence, sentence_triples) in enumerate(paper_triples):\n",
    "        total_triples += len(sentence_triples)\n",
    "\n",
    "        for triple in sentence_triples[:]:\n",
    "            if is_valid_triple(triple):\n",
    "                s, p, o = clean_triple(triple)\n",
    "                unique_s[s] += 1\n",
    "                unique_p[p] += 1\n",
    "                unique_o[o] += 1\n",
    "            else:\n",
    "                bad_triples.append(triple)\n",
    "                sentence_triples.remove(triple)\n",
    "\n",
    "    return [triple for (_, sentence_triples) in paper_triples for triple in sentence_triples], total_triples, len(unique_s), len(unique_p), len(unique_o), len(bad_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5297ef-8f8f-436d-9048-008e1ea4c9a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test the context strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d9603-615d-4e26-8754-229b66559cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_triples_for_sentences(context, sentences, n_samples, LLM, webnlg_dataset):\n",
    "    \"\"\"\n",
    "    Prompts and returns the triples from the LLM\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt = webnlg_dataset.get_prompts(context, sentences, n_samples, avoid_explanations=True)\n",
    "\n",
    "    return LLM.get_triples(system_prompt, user_prompt, allow_bad_triples=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_triples_from_papers(papers,\n",
    "                            n_samples,\n",
    "                            sentences_per_prompt,\n",
    "                            overlap,\n",
    "                            prompt_strategy : PromptStrategy,\n",
    "                            LLM,\n",
    "                            main_dataset):\n",
    "    \"\"\"\n",
    "    Given a JSON response containing paper abstracts from https://api.plos.org, \n",
    "    returns a dict of paper ID (DOI) -> list of triples\n",
    "    \"\"\"\n",
    "    paper_triples = dict()\n",
    "    \n",
    "    for i, paper in enumerate(pbar := tqdm(papers[\"response\"][\"docs\"], file=open(\"progress.log\", \"w\"))):\n",
    "        paper_id = paper[\"id\"]\n",
    "        paper_triples[paper_id] = []\n",
    "\n",
    "        abstract_sentences = nltk.sent_tokenize(paper[\"abstract\"][0])\n",
    "\n",
    "        # Group the paper sentences into tuples of sentences_per_prompt sentences, with the\n",
    "        # desired overlapping. It will also add a context depending on the strategy being used\n",
    "        sentence_chunks = []\n",
    "        if prompt_strategy == PromptStrategy.PREVIOUS_SENTENCES:\n",
    "            # And prepare the next chunk to have a correct starting overlap\n",
    "            if overlap > 0:\n",
    "                # Add the first group of sentences manually, as they will have no context\n",
    "                sentence_chunks.append((\"There is no context for this sample\", \" \".join(abstract_sentences[:sentences_per_prompt])))\n",
    "                # And prepare for the next iterations\n",
    "                abstract_sentences = abstract_sentences[sentences_per_prompt - overlap:]\n",
    "\n",
    "            for sentences_chunk in list(mit.windowed(abstract_sentences, n=sentences_per_prompt, step=sentences_per_prompt-overlap)):\n",
    "                sentence_chunks.append((\" \".join(filter(None, sentences_chunk[:overlap])), \" \".join(filter(None, sentences_chunk[overlap:]))))\n",
    "\n",
    "        elif prompt_strategy == PromptStrategy.SECTION_CONTENTS:\n",
    "            sentence_chunks = [(None, paper[\"abstract\"])]\n",
    "\n",
    "        for j, (context, sentences_chunk) in enumerate(sentence_chunks):\n",
    "            pbar.set_description(f\"sentences_per_prompt: {sentences_per_prompt}, strategy: {prompt_strategy}, overlap: {overlap}. Generating triples for paper {i+1}/{len(papers[\"response\"][\"docs\"])}, sentence chunk {j+1}/{len(sentence_chunks)}\")\n",
    "\n",
    "            sentence_triples = get_triples_for_sentences(context, sentences_chunk, n_samples, LLM, main_dataset)\n",
    "            paper_triples[paper_id] += [(sentences_chunk, sentence_triples)]\n",
    "\n",
    "    return paper_triples\n",
    "\n",
    "def get_triples_from_docred(n_samples,\n",
    "                            sentences_per_prompt,\n",
    "                            overlap,\n",
    "                            prompt_strategy : PromptStrategy,\n",
    "                            LLM,\n",
    "                            main_dataset, \n",
    "                            webnlg_dataset):\n",
    "    \"\"\"\n",
    "    Given a docRED dataset parsed from https://huggingface.co/datasets/thunlp/docred, return a dict of text -> list of triples\n",
    "    \"\"\"\n",
    "    docred_triples = dict()\n",
    "\n",
    "    # First 100 validation samples, which contain both text and ground truth triples (to evaluate later on)\n",
    "    for i, docred_sample in enumerate(pbar := tqdm(main_dataset.docred[\"validation\"].select(range(100)), file=open(\"progress.log\", \"w\"))):\n",
    "        text, _ = main_dataset.get_text_and_triples(docred_sample)\n",
    "\n",
    "        sample_id = text\n",
    "        docred_triples[sample_id] = []\n",
    "\n",
    "        sample_sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "        # Group the paper sentences into tuples of sentences_per_prompt sentences, with the\n",
    "        # desired overlapping. It will also add a context depending on the strategy being used\n",
    "        sentence_chunks = []\n",
    "        if prompt_strategy == PromptStrategy.PREVIOUS_SENTENCES:\n",
    "            # And prepare the next chunk to have a correct starting overlap\n",
    "            if overlap > 0:\n",
    "                # Add the first group of sentences manually, as they will have no context\n",
    "                sentence_chunks.append((\"There is no context for this sample\", \" \".join(sample_sentences[:sentences_per_prompt])))\n",
    "                # And prepare for the next iterations\n",
    "                abstract_sentences = sample_sentences[sentences_per_prompt - overlap:]\n",
    "\n",
    "            for sentences_chunk in list(mit.windowed(sample_sentences, n=sentences_per_prompt, step=sentences_per_prompt-overlap)):\n",
    "                sentence_chunks.append((\" \".join(filter(None, sentences_chunk[:overlap])), \" \".join(filter(None, sentences_chunk[overlap:]))))\n",
    "\n",
    "        elif prompt_strategy == PromptStrategy.SECTION_CONTENTS:\n",
    "            sentence_chunks = [(None, text)]\n",
    "\n",
    "        for j, (context, sentences_chunk) in enumerate(sentence_chunks):\n",
    "            pbar.set_description(f\"sentences_per_prompt: {sentences_per_prompt}, strategy: {prompt_strategy}, overlap: {overlap}. Generating triples for paper {i+1}/100, sentence chunk {j+1}/{len(sentence_chunks)}\")\n",
    "\n",
    "            sentence_triples = get_triples_for_sentences(context, sentences_chunk, n_samples, LLM, webnlg_dataset)\n",
    "            docred_triples[sample_id] += [(sentences_chunk, sentence_triples)]\n",
    "\n",
    "    return docred_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e02c7-4abe-47bf-987c-0cafe5deb043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiments(papers,\n",
    "                    n_samples,\n",
    "                    sentences_per_prompt,\n",
    "                    overlap,\n",
    "                    prompt_strategy,\n",
    "                    LLM,\n",
    "                    main_dataset,\n",
    "                    webnlg_dataset):\n",
    "    if isinstance(main_dataset, Datasets.WebNLGDataset):\n",
    "        paper_triples = get_triples_from_papers(papers,\n",
    "                                                n_samples,\n",
    "                                                sentences_per_prompt,\n",
    "                                                overlap,\n",
    "                                                prompt_strategy,\n",
    "                                                LLM,\n",
    "                                                main_dataset)\n",
    "        results = {\n",
    "           \"n_samples\": [],\n",
    "           \"prompt_strategy\": [],\n",
    "           \"sentences_per_prompt\": [],\n",
    "           \"overlap\": [],\n",
    "\n",
    "           \"paper_id\": [],\n",
    "           \"clean_triples\": [],\n",
    "           \"total_triples\": [],\n",
    "           \"unique_s\": [],\n",
    "           \"unique_p\": [],\n",
    "           \"unique_o\": [],\n",
    "           \"bad_triples\": [],\n",
    "        }\n",
    "\n",
    "        for paper_id, triples in paper_triples.items():\n",
    "            clean_triples, total_triples, unique_s, unique_p, unique_o, bad_triples = clean_and_get_triples_stats(triples)\n",
    "\n",
    "            results[\"n_samples\"].append(n_samples)\n",
    "            results[\"prompt_strategy\"].append(prompt_strategy)\n",
    "            results[\"sentences_per_prompt\"].append(sentences_per_prompt)\n",
    "            results[\"overlap\"].append(overlap)\n",
    "\n",
    "            results[\"paper_id\"].append(paper_id)\n",
    "            results[\"clean_triples\"].append(clean_triples)\n",
    "            results[\"total_triples\"].append(total_triples)\n",
    "            results[\"unique_s\"].append(unique_s)\n",
    "            results[\"unique_p\"].append(unique_p)\n",
    "            results[\"unique_o\"].append(unique_o)\n",
    "            results[\"bad_triples\"].append(bad_triples)\n",
    "    else:\n",
    "        docred_triples = get_triples_from_docred(n_samples,\n",
    "                                                 sentences_per_prompt,\n",
    "                                                 overlap,\n",
    "                                                 prompt_strategy,\n",
    "                                                 LLM,\n",
    "                                                 main_dataset,\n",
    "                                                 webnlg_dataset)\n",
    "\n",
    "        results = {\n",
    "           \"n_samples\": [],\n",
    "           \"prompt_strategy\": [],\n",
    "           \"sentences_per_prompt\": [],\n",
    "           \"overlap\": [],\n",
    "\n",
    "           \"text\": [],\n",
    "           \"clean_triples\": [],\n",
    "           \"total_triples\": [],\n",
    "           \"unique_s\": [],\n",
    "           \"unique_p\": [],\n",
    "           \"unique_o\": [],\n",
    "           \"bad_triples\": [],\n",
    "        }\n",
    "\n",
    "        for text, triples in docred_triples.items():\n",
    "            clean_triples, total_triples, unique_s, unique_p, unique_o, bad_triples = clean_and_get_triples_stats(triples)\n",
    "\n",
    "            results[\"n_samples\"].append(n_samples)\n",
    "            results[\"prompt_strategy\"].append(prompt_strategy)\n",
    "            results[\"sentences_per_prompt\"].append(sentences_per_prompt)\n",
    "            results[\"overlap\"].append(overlap)\n",
    "\n",
    "            results[\"text\"].append(text)\n",
    "            results[\"clean_triples\"].append(clean_triples)\n",
    "            results[\"total_triples\"].append(total_triples)\n",
    "            results[\"unique_s\"].append(unique_s)\n",
    "            results[\"unique_p\"].append(unique_p)\n",
    "            results[\"unique_o\"].append(unique_o)\n",
    "            results[\"bad_triples\"].append(bad_triples)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e1deb-7638-47b2-a15c-53566f702d9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ecc0f-a310-493c-a861-967118a39267",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check variability across strategies and parameters\n",
    "Depending on the number of sentences we ask the triples for in a given prompt, the context length and the context strategies themselves, the amount of triples generated can vary a lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1472fc-0546-40c2-b937-60755370aecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"paper_triples_results.csv\"): # Resume the experiments\n",
    "    results = pd.read_csv(\"paper_triples_results.csv\")\n",
    "else:\n",
    "    if isinstance(main_dataset, Datasets.WebNLGDataset):\n",
    "        results = pd.DataFrame({\n",
    "            \"n_samples\": [],\n",
    "            \"prompt_strategy\": [],\n",
    "            \"sentences_per_prompt\": [],\n",
    "            \"overlap\": [],\n",
    "\n",
    "            \"paper_id\": [],\n",
    "            \"clean_triples\": [],\n",
    "            \"total_triples\": [],\n",
    "            \"unique_s\": [],\n",
    "            \"unique_p\": [],\n",
    "            \"unique_o\": [],\n",
    "            \"bad_triples\": []\n",
    "        })\n",
    "    else:\n",
    "        results = pd.DataFrame({\n",
    "            \"n_samples\": [],\n",
    "            \"prompt_strategy\": [],\n",
    "            \"sentences_per_prompt\": [],\n",
    "            \"overlap\": [],\n",
    "\n",
    "            \"text\": [],\n",
    "            \"clean_triples\": [],\n",
    "            \"total_triples\": [],\n",
    "            \"unique_s\": [],\n",
    "            \"unique_p\": [],\n",
    "            \"unique_o\": [],\n",
    "            \"bad_triples\": []\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08dfd3-389b-4a92-9104-f24a3a7dba0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exists_result_with_config(n_samples, prompt_strategy, sentences_per_prompt, overlap):\n",
    "    return (\n",
    "        (results[\"n_samples\"] == float(n_samples)) &\n",
    "        (results[\"prompt_strategy\"] == str(prompt_strategy)) &\n",
    "        (results[\"sentences_per_prompt\"] == float(sentences_per_prompt)) &\n",
    "        (results[\"overlap\"] == float(overlap))\n",
    "    ).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b50a3-c304-4fa0-ae44-91c0520a44d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 8 # WebNLG samples to use\n",
    "max_sentences_per_prompt = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02709f9-b0ba-47ce-a74f-a7d8a5290c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not exists_result_with_config(n_samples, PromptStrategy.SECTION_CONTENTS, 0, 0):\n",
    "    results_contents = run_experiments(papers,\n",
    "                                       n_samples,\n",
    "                                       0,\n",
    "                                       0,  # We don't care about overlap in this case\n",
    "                                       PromptStrategy.SECTION_CONTENTS,\n",
    "                                       LLM,\n",
    "                                       main_dataset,\n",
    "                                       webnlg_dataset)\n",
    "    results = pd.concat([results, pd.DataFrame(results_contents)], ignore_index = True)\n",
    "    results.to_csv(\"paper_triples_results.csv\", index=False)\n",
    "else:\n",
    "    print(\"Test Skipped:\", n_samples, PromptStrategy.SECTION_CONTENTS, 0, 0)\n",
    "\n",
    "for i in range(max_sentences_per_prompt): # From 1 to max_sentences_per_prompt sentences at once\n",
    "    sentences_per_prompt = i+1\n",
    "\n",
    "    if not exists_result_with_config(n_samples, PromptStrategy.PREVIOUS_SENTENCES, sentences_per_prompt, 0):\n",
    "        results_sentences_no_overlap = run_experiments(papers,\n",
    "                                                       n_samples,\n",
    "                                                       sentences_per_prompt,\n",
    "                                                       0, # We don't care about overlap in this case\n",
    "                                                       PromptStrategy.PREVIOUS_SENTENCES,\n",
    "                                                       LLM,\n",
    "                                                       main_dataset,\n",
    "                                                       webnlg_dataset)\n",
    "\n",
    "        results = pd.concat([results, pd.DataFrame(results_sentences_no_overlap)], ignore_index = True)\n",
    "        results.to_csv(\"paper_triples_results.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Test Skipped:\", n_samples, PromptStrategy.PREVIOUS_SENTENCES, sentences_per_prompt, 0)\n",
    "\n",
    "    for j in range(max_sentences_per_prompt): # From 1 to max_sentences_per_prompt context sentences\n",
    "        overlap = j+1\n",
    "        if overlap >= sentences_per_prompt:\n",
    "            continue\n",
    "\n",
    "        if not exists_result_with_config(n_samples, PromptStrategy.PREVIOUS_SENTENCES, sentences_per_prompt, overlap):\n",
    "            results_sentences = run_experiments(papers,\n",
    "                                                n_samples,\n",
    "                                                sentences_per_prompt,\n",
    "                                                overlap,\n",
    "                                                PromptStrategy.PREVIOUS_SENTENCES,\n",
    "                                                LLM,\n",
    "                                                main_dataset,\n",
    "                                                webnlg_dataset)\n",
    "            results = pd.concat([results, pd.DataFrame(results_sentences)], ignore_index = True)\n",
    "            results.to_csv(\"paper_triples_results.csv\", index=False)\n",
    "        else:\n",
    "            print(\"Test Skipped:\", n_samples, PromptStrategy.PREVIOUS_SENTENCES, sentences_per_prompt, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c51ca-0127-4229-81e7-bd1b9bffc377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"paper_triples_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a7c6bb-1942-496e-85f5-311df83d15a0",
   "metadata": {},
   "source": [
    "## Save the averaged results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448faaab-24e8-4370-8342-c49d9ed019e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isinstance(main_dataset, Datasets.WebNLGDataset):\n",
    "    results = results.drop(columns='paper_id')\n",
    "else:\n",
    "    results = results.drop(columns='text')\n",
    "results = results.drop(columns='clean_triples')\n",
    "results = results.drop(columns='n_samples')\n",
    "\n",
    "results = results.groupby(['prompt_strategy',\n",
    "                           'sentences_per_prompt',\n",
    "                           'overlap']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10afd40-d712-4596-8693-58ddff815506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"paper_triples_results_clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [jrcACLJEOLAB]",
   "language": "python",
   "name": "conda-env-jrcACLJEOLAB-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
